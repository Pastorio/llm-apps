{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "base_project_path = os.getcwd().split(os.environ['PROJECT_NAME'])[0] + os.environ['PROJECT_NAME'] \n",
    "\n",
    "from langchain.schema import AIMessage, HumanMessage, SystemMessage\n",
    "from langchain.chat_models import ChatOpenAI\n",
    "from langchain.llms import OpenAI\n",
    "from langchain import FAISS\n",
    "\n",
    "from langchain.prompts import PromptTemplate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Conversation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exploding and vanishing gradients are common issues that can occur during the training of deep neural networks, particularly in recurrent neural networks (RNNs). These issues arise due to the nature of backpropagation, where gradients are propagated backward through the network to update the weights.\n",
      "\n",
      "Exploding gradients refer to the situation when the gradients grow exponentially as they are propagated backward through the network layers. This can lead to large updates to the weights, causing instability in the learning process. In extreme cases, the weights can become so large that they overflow, resulting in NaN (Not a Number) values and rendering the model useless.\n",
      "\n",
      "On the other hand, vanishing gradients occur when the gradients diminish exponentially as they are propagated backward through the network layers. This means that the updates to the weights become very small, and the network fails to learn effectively. The problem becomes more pronounced in deep networks with many layers, as the gradients have to pass through multiple layers, each potentially multiplying or diminishing the gradients.\n",
      "\n",
      "Both exploding and vanishing gradients can hinder the convergence of the network and make it difficult to train deep neural networks effectively.\n",
      "\n",
      "There are several reasons why exploding and vanishing gradients can occur:\n",
      "\n",
      "1. Activation functions: Certain activation functions, such as sigmoid or hyperbolic tangent, can amplify or diminish the gradients. The sigmoid function, for example, has a derivative that is close to zero for large inputs, leading to vanishing gradients.\n",
      "\n",
      "2. Weight initialization: If the weights are initialized with large values, it can lead to exploding gradients. Conversely, if the weights are initialized with small values, it can result in vanishing gradients.\n",
      "\n",
      "3. Network architecture: Deep networks with many layers can exacerbate the problem of vanishing gradients. As the gradients are multiplied through each layer during backpropagation, they can diminish exponentially.\n",
      "\n",
      "To mitigate the issues of exploding and vanishing gradients, several techniques can be employed:\n",
      "\n",
      "1. Gradient clipping: This technique involves capping the gradients to a maximum value during training. By limiting the magnitude of the gradients, it prevents them from growing too large and causing instability.\n",
      "\n",
      "2. Weight initialization: Careful initialization of the weights can help alleviate the problem. Techniques like Xavier or He initialization, which take into account the number of input and output connections of a layer, can help in balancing the gradients.\n",
      "\n",
      "3. Activation functions: Using activation functions that do not suffer from the vanishing gradient problem, such as ReLU (Rectified Linear Unit), can help in reducing the impact of vanishing gradients.\n",
      "\n",
      "4. Batch normalization: Applying batch normalization to the network can help stabilize the gradients by normalizing the inputs to each layer.\n",
      "\n",
      "5. Residual connections: Introducing skip connections or residual connections in the network architecture, as in the case of residual neural networks (ResNets), can help in mitigating the vanishing gradient problem by allowing the gradients to flow directly through the skip connections.\n",
      "\n",
      "By employing these techniques, the issues of exploding and vanishing gradients can be minimized, allowing for more stable and effective training of deep neural networks.\n"
     ]
    }
   ],
   "source": [
    "# Chat Model\n",
    "chat = ChatOpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"], \n",
    "                  model_name=\"gpt-3.5-turbo\", \n",
    "                  temperature=0.3)\n",
    "\n",
    "# Messages\n",
    "messages = [\n",
    "    SystemMessage(content=\"You are expert in data science and machine learning, aswer the following questions with a detailed thecnical answer:\"),\n",
    "    HumanMessage(content=\"What is exploding/vanishing gradients\")\n",
    "]\n",
    "\n",
    "# Chat\n",
    "response=chat(messages)\n",
    "print(response.content,end='\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chat Conversational Memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- ConversationBufferMemory    \n",
    "- ConversationSummaryMemory\n",
    "- ConversationBufferWindowMemory\n",
    "- ConversationSummaryBufferMemory\n",
    "- ConversationKnowledgeGraphMemory \n",
    "- ConversationEntityMemory\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'My name is John!', 'history': '', 'response': \"Hello John! It's nice to meet you. How can I assist you today?\"}\n",
      "{'input': 'What is my name?', 'history': \"Human: My name is John!\\nAI: Hello John! It's nice to meet you. How can I assist you today?\", 'response': 'Your name is John.'}\n"
     ]
    }
   ],
   "source": [
    "from langchain.chains import ConversationChain\n",
    "from langchain.chains.conversation.memory import ConversationBufferMemory\n",
    "\n",
    "model = ChatOpenAI(openai_api_key=os.environ[\"OPENAI_API_KEY\"], model_name=\"gpt-3.5-turbo\", temperature=0.3)\n",
    "\n",
    "# Chain without memory\n",
    "chain = ConversationChain(llm=model)\n",
    "print(chain(\"My name is John!\"))\n",
    "print(chain(\"What is my name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'input': 'Good morning AI, my name is John!', 'history': '', 'response': \"Good morning John! It's nice to meet you. How can I assist you today?\"}\n",
      "{'input': 'What is my name?', 'history': \"Human: Good morning AI, my name is John!\\nAI: Good morning John! It's nice to meet you. How can I assist you today?\", 'response': 'Your name is John.'}\n"
     ]
    }
   ],
   "source": [
    "# Chain with buffer memory\n",
    "chain_buff = ConversationChain(\n",
    "    llm=model,\n",
    "    memory=ConversationBufferMemory()\n",
    ")\n",
    "print(chain_buff(\"My name is John!\"))\n",
    "print(chain_buff(\"What is my name?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spent a total of 137 tokens\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'Yes, I know. Your name is John.'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.callbacks import get_openai_callback\n",
    "\n",
    "def count_tokens(chain, query):\n",
    "    with get_openai_callback() as cb:\n",
    "        result = chain.run(query)\n",
    "        print(f'Spent a total of {cb.total_tokens} tokens')\n",
    "\n",
    "    return result\n",
    "\n",
    "count_tokens(chain_buff, \"My name is John!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### load_qa_chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import PyPDF2\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "from langchain import FAISS\n",
    "\n",
    "# Load PyPDF2\n",
    "def read_pdf(filename):\n",
    "    with open(filename, 'rb') as f:\n",
    "        pdf_reader = PyPDF2.PdfReader(f)\n",
    "        text = ''\n",
    "        for page in pdf_reader.pages:\n",
    "            text += page.extract_text()\n",
    "    return text\n",
    "\n",
    "pdf_path = base_project_path + '\\\\data\\\\raw\\\\ml_books\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf'\n",
    "text = read_pdf(pdf_path)\n",
    "\n",
    "# Split in chunks using CharacterTextSplitter\n",
    "text_splitter = CharacterTextSplitter(\n",
    "    separator=\"\\n\",\n",
    "    chunk_size=1000,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len\n",
    ")\n",
    "all_splits = text_splitter.split_text(text)\n",
    "\n",
    "# Convert the chunks of text into embeddings to form a knowledge base\n",
    "embeddings = OpenAIEmbeddings()\n",
    "knowledgeBase = FAISS.from_texts(all_splits, embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The optimizer used in the described model is Adam optimizer. Adam is a popular optimization algorithm that is commonly used in deep learning. It is known for its efficiency and ability to handle large datasets.\\n\\nThe specific hyperparameters used for Adam in this model are:\\n- β1 = 0.9\\n- β2 = 0.98\\n- ɛ = 10^-9\\n\\nThe learning rate is varied during training according to a specific formula:\\nlrate = d * (step_num^(-0.5)) * min(step_num^(-0.5), step_num^(-1.5) * warmup_steps^(-0.5))\\n\\nIn this formula, d is a constant, step_num represents the current training step, and warmup_steps is a predefined value of 4000.\\n\\nThis learning rate schedule increases the learning rate linearly for the first warmup_steps training steps and then decreases it proportionally to the inverse square root of the step number.\\n\\nOverall, the Adam optimizer with this learning rate schedule helps in efficiently optimizing the model's parameters during training.\""
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create the model\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "\n",
    "# Use OpenAI as the LLM\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "chain = load_qa_chain(llm=model, chain_type=\"stuff\")\n",
    "\n",
    "question = \"Explain optimizer for the text\"\n",
    "docs = knowledgeBase.similarity_search(question)\n",
    "\n",
    "chain.run(input_documents=docs, question=question)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### RetrievalQA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load PyPDFLoader\n",
    "pdf_path = base_project_path + '\\\\data\\\\raw\\\\ml_books\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "text = loader.load()\n",
    "\n",
    "# Split in chunks using RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "# all_splits = text_splitter.split_documents(text)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "# Convert the chunks of text into embeddings to form a knowledge base\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# knowledgeBase = FAISS.from_texts(all_splits, embeddings)\n",
    "\n",
    "persist_directory = 'docs/chroma/'\n",
    "knowledgeBase = Chroma.from_documents(documents=all_splits,\n",
    "                                      embedding=embeddings,\n",
    "                                      persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'query': 'Explain optimizer for the text',\n",
       " 'result': \"The optimizer used in the text is called Adam. Adam is a popular optimization algorithm commonly used in deep learning. It is an extension of stochastic gradient descent (SGD) that combines the advantages of both adaptive learning rate methods and momentum methods.\\n\\nIn the text, the specific hyperparameters used for Adam are β1= 0.9, β2= 0.98, and ϵ= 10−9. These values determine the exponential decay rates for the first and second moments of the gradients, as well as a small constant added to the denominator to prevent division by zero.\\n\\nThe learning rate is varied over the course of training using a formula described in equation (3) in the text. The learning rate (lrate) is determined by the value of d, the step number (step_num), and the warmup_steps. The warmup_steps is set to 4000.\\n\\nThe formula for the learning rate increases it linearly for the first warmup_steps training steps and then decreases it proportionally to the inverse square root of the step number. This approach helps the model to converge faster initially and then fine-tune the learning rate as training progresses.\\n\\nOverall, the Adam optimizer with these specific hyperparameters and learning rate schedule is used to update the model's parameters during training and optimize the model's performance.\"}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Use OpenAI as the LLM\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "from langchain.chains import RetrievalQA\n",
    "\n",
    "qa_chain = RetrievalQA.from_chain_type(model,\n",
    "                                       retriever=knowledgeBase.as_retriever())\n",
    "\n",
    "question = \"Explain optimizer for the text\"\n",
    "result = qa_chain({\"query\": question})\n",
    "\n",
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VectorstoreIndexCreator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The optimizer used in the Transformer model is called Adam. Adam stands for Adaptive Moment Estimation and it is a popular optimization algorithm for training neural networks.\\n\\nAdam combines the ideas of two other optimization algorithms: AdaGrad and RMSprop. It maintains a running average of the past gradients and squared gradients, and uses these averages to update the parameters of the model.\\n\\nThe Adam optimizer uses two main hyperparameters: beta1 and beta2. Beta1 controls the decay rate of the running average of the gradients, while beta2 controls the decay rate of the running average of the squared gradients.\\n\\nDuring training, the learning rate is varied according to a schedule. In the case of the Transformer model, the learning rate is increased linearly for the first warmup_steps training steps, and then decreased proportionally to the inverse square root of the step number.\\n\\nOverall, the Adam optimizer helps to efficiently update the parameters of the Transformer model during training, leading to faster convergence and better performance.'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain.indexes import VectorstoreIndexCreator\n",
    "# Load PyPDFLoader\n",
    "pdf_path = base_project_path + '\\\\data\\\\raw\\\\ml_books\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "\n",
    "# Use VectorstoreIndexCreator to generate the chain\n",
    "index = VectorstoreIndexCreator(\n",
    "\n",
    "    text_splitter=CharacterTextSplitter(chunk_size=1000, chunk_overlap=0),\n",
    "\n",
    "    embedding=OpenAIEmbeddings(),\n",
    "\n",
    "    vectorstore_cls=Chroma\n",
    "    \n",
    ").from_loaders([loader])\n",
    "\n",
    "query = \"Explain optimizer for the text\"\n",
    "\n",
    "index.query(llm=model, question=query, chain_type=\"stuff\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ConversationalRetrievalChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "# Load PyPDFLoader\n",
    "pdf_path = base_project_path + '\\\\data\\\\raw\\\\ml_books\\\\NIPS-2017-attention-is-all-you-need-Paper.pdf'\n",
    "loader = PyPDFLoader(pdf_path)\n",
    "text = loader.load()\n",
    "\n",
    "# Split in chunks using RecursiveCharacterTextSplitter\n",
    "# text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "# all_splits = text_splitter.split_documents(text)\n",
    "\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=20)\n",
    "all_splits = text_splitter.split_documents(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.vectorstores import Chroma\n",
    "# Convert the chunks of text into embeddings to form a knowledge base\n",
    "embeddings = OpenAIEmbeddings()\n",
    "\n",
    "# knowledgeBase = FAISS.from_texts(all_splits, embeddings)\n",
    "\n",
    "persist_directory = 'docs/chroma/'\n",
    "knowledgeBase = Chroma.from_documents(documents=all_splits,\n",
    "                                      embedding=embeddings,\n",
    "                                      persist_directory=persist_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI as the LLM\n",
    "model = ChatOpenAI(\n",
    "    openai_api_key=os.environ[\"OPENAI_API_KEY\"],\n",
    "    model_name=\"gpt-3.5-turbo\",\n",
    "    temperature=0.3,\n",
    ")\n",
    "\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "qa = ConversationalRetrievalChain.from_llm(model, knowledgeBase.as_retriever())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Transformer model architecture consists of two main components: the encoder and the decoder.\\n\\nThe encoder is composed of a stack of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The self-attention mechanism allows the encoder to weigh the importance of different words in the input sequence when encoding information. The feed-forward network helps to capture complex relationships between words. Each sub-layer is followed by a residual connection and layer normalization. The output of each sub-layer is then added to the input, and the result is normalized.\\n\\nThe decoder also consists of a stack of identical layers. In addition to the two sub-layers present in each encoder layer, the decoder inserts a third sub-layer. This additional sub-layer performs multi-head attention over the output of the encoder stack. This allows the decoder to focus on different parts of the input sequence when generating the output. Similar to the encoder, the decoder also uses residual connections and layer normalization.\\n\\nTo prevent the decoder from attending to subsequent positions, the self-attention sub-layer in the decoder stack is modified. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for each position only depend on the known outputs at positions before it.\\n\\nOverall, the Transformer model architecture utilizes self-attention mechanisms and feed-forward networks to encode and decode information, allowing it to capture complex relationships and generate accurate outputs.'"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = []\n",
    "query = \"Explain the transformer model archtiecture\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result[\"answer\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"The optimizer plays a crucial role in text processing by adjusting the model's parameters during training to minimize the loss function. It determines how the model learns and updates its weights based on the gradients calculated during backpropagation. In the given context, the Adam optimizer with specific hyperparameters (β1, β2, and ϵ) was used to train the models. The learning rate was also varied over the course of training using a formula that increased it linearly for the first warmup_steps training steps and then decreased it proportionally to the inverse square root of the step number. This optimization technique helps the model converge to better performance and improve its ability to generate accurate and meaningful text.\""
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history = [(query, result[\"answer\"])]\n",
    "query = \"Explain optimizer for the text\"\n",
    "result = qa({\"question\": query, \"chat_history\": chat_history})\n",
    "result['answer']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Explain the transformer model archtiecture',\n",
       "  'The Transformer model architecture consists of two main components: the encoder and the decoder.\\n\\nThe encoder is composed of a stack of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The self-attention mechanism allows the encoder to weigh the importance of different words in the input sequence when encoding information. The feed-forward network helps to capture complex relationships between words. Each sub-layer is followed by a residual connection and layer normalization. The output of each sub-layer is then added to the input, and the result is normalized.\\n\\nThe decoder also consists of a stack of identical layers. In addition to the two sub-layers present in each encoder layer, the decoder inserts a third sub-layer. This additional sub-layer performs multi-head attention over the output of the encoder stack. This allows the decoder to focus on different parts of the input sequence when generating the output. Similar to the encoder, the decoder also uses residual connections and layer normalization.\\n\\nTo prevent the decoder from attending to subsequent positions, the self-attention sub-layer in the decoder stack is modified. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for each position only depend on the known outputs at positions before it.\\n\\nOverall, the Transformer model architecture utilizes self-attention mechanisms and feed-forward networks to encode and decode information, allowing it to capture complex relationships and generate accurate outputs.')]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history.append([(query, result[\"answer\"])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Explain the transformer model archtiecture',\n",
       "  'The Transformer model architecture consists of two main components: the encoder and the decoder.\\n\\nThe encoder is composed of a stack of identical layers. Each layer has two sub-layers: a multi-head self-attention mechanism and a position-wise fully connected feed-forward network. The self-attention mechanism allows the encoder to weigh the importance of different words in the input sequence when encoding information. The feed-forward network helps to capture complex relationships between words. Each sub-layer is followed by a residual connection and layer normalization. The output of each sub-layer is then added to the input, and the result is normalized.\\n\\nThe decoder also consists of a stack of identical layers. In addition to the two sub-layers present in each encoder layer, the decoder inserts a third sub-layer. This additional sub-layer performs multi-head attention over the output of the encoder stack. This allows the decoder to focus on different parts of the input sequence when generating the output. Similar to the encoder, the decoder also uses residual connections and layer normalization.\\n\\nTo prevent the decoder from attending to subsequent positions, the self-attention sub-layer in the decoder stack is modified. This masking, combined with the fact that the output embeddings are offset by one position, ensures that the predictions for each position only depend on the known outputs at positions before it.\\n\\nOverall, the Transformer model architecture utilizes self-attention mechanisms and feed-forward networks to encode and decode information, allowing it to capture complex relationships and generate accurate outputs.'),\n",
       " [('Explain optimizer for the text',\n",
       "   \"The optimizer plays a crucial role in text processing by adjusting the model's parameters during training to minimize the loss function. It determines how the model learns and updates its weights based on the gradients calculated during backpropagation. In the given context, the Adam optimizer with specific hyperparameters (β1, β2, and ϵ) was used to train the models. The learning rate was also varied over the course of training using a formula that increased it linearly for the first warmup_steps training steps and then decreased it proportionally to the inverse square root of the step number. This optimization technique helps the model converge to better performance and improve its ability to generate accurate and meaningful text.\")]]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chat_history"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
